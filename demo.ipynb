{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational Graph Convolutional Networks\n",
    "\n",
    "Here are some experimentations on R-GCNs, based on the paper *Modeling Relational Data with Graph Convolutional Networks*  [[Schlichtkrull *et al.*]](https://arxiv.org/pdf/1703.06103.pdf).\n",
    "\n",
    "Load the [FB15K-237](https://www.microsoft.com/en-us/download/details.aspx?id=52312) dataset, from [[Toutanova et al. EMNLP 2015]](http://dx.doi.org/10.18653/v1/D15-1174). You can pass `download_if_absent=True` for automatic downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272115 training triples found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "import fb15k\n",
    "\n",
    "train, val, test = fb15k.load(\"train\", \"valid\", \"test\")\n",
    "print(f\"{len(train)} training triples found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the adjacency tensor from the training data. Let $R = |\\mathcal{R}|$ and $E = |\\mathcal{E}|$ the number of relations and entities in the knowledge graph. The adjacency tensor is a list of tensors $\\mathcal{T} = (T^{(1)}, \\ldots, T^{(R)}$, with $T^{(i)} \\in \\mathbb{R}^{E \\times E}$ defined by:\n",
    "$$\n",
    "T^{(i)} = (D^{(i)})^{-1} \\times A^{(i)}\n",
    "$$\n",
    "With $A$ the canonical adjacency tensor:\n",
    "\\begin{equation}\n",
    "    A_{kj}^{(i)} =\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ (e_k, r_i, e_j) \\text{ is in the graph} \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "  \\end{equation}\n",
    " And $D^{(i)}$ a diagonal matrix containing the in-degree of each entity in the graph, as described in [[Kipf & Welling 2016]](https://arxiv.org/abs/1609.02907). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14505 entities and 237 relations found.\n"
     ]
    }
   ],
   "source": [
    "from utils import graph\n",
    "\n",
    "T, e2c, e2i, r2i = graph.build_adjacency_tensor(train)\n",
    "n_relations = len(T)\n",
    "n_entities = T[0].shape[0]\n",
    "\n",
    "print(f\"{n_entities} entities and {n_relations} relations found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 distinct classes found.\n"
     ]
    }
   ],
   "source": [
    "# Supervised setting: each entity has a class. Here we build the ground truth, that is the expected output tensor\n",
    "# Give a unique identifier to each class\n",
    "classes = {c: i for i, c in enumerate(set(e2c))}\n",
    "n_classes = len(classes)\n",
    "y_true = [classes[c] for c in e2c]\n",
    "y_true = torch.LongTensor(y_true)\n",
    "print(f\"{n_classes} distinct classes found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the R-GCN network. The equation for each convolution layer is:\n",
    "$$H_i^{(l+1)} = \\sigma\\left( \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_i^r} \\frac{1}{c_{i,r}} W_r^{(l)}H_j^{(l)} + W_0^{(l)} H_i^{(l)} \\right)$$\n",
    "\n",
    "with $H_i^{(l)} \\in \\mathbb{R}^{d^{(l)}}$ the hidden state of node $e_i$.\n",
    "\n",
    "We use *basis decomposition* rather than *block-diagonal decomposition*. Each weight matrix is a linear combination of $B$ basis functions:\n",
    "$$\n",
    "W_r^{(l)} = \\sum_{b=1}^B A_{rb}^{(l)} V_b^{(l)}\n",
    "$$\n",
    "\n",
    "and $V_b^{(l)} \\in \\mathbb{R}^{d^{(l+1)} \\times d^{(l)}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGCN(\n",
      "  (convolutions): ModuleList(\n",
      "    (0): RGCNLayer()\n",
      "    (1): RGCNLayer()\n",
      "    (2): RGCNLayer()\n",
      "    (3): RGCNLayer()\n",
      "  )\n",
      "  (softmax): Softmax(dim=0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from rgcn import RGCN\n",
    "\n",
    "rgcn = RGCN(T,\n",
    "            n_classes=n_classes,\n",
    "            hidden_sizes=[64, 32, 16],\n",
    "            n_basis=20\n",
    "            )\n",
    "\n",
    "print(rgcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we feed it the featureless representation of the nodes, which is simply the identify matrix of $\\mathbb{E \\times E}$. We use an Adam optimizer with cross-entropy loss.\n",
    "\n",
    "⚠️ *The use of `torch.sparse.LongTensor` yields the following error: `RuntimeError: sparse_.is_sparse() INTERNAL ASSERT FAILED`, both on CPU and GPU.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import tensor\n",
    "\n",
    "optim = torch.optim.Adam(rgcn.parameters(recurse=True), lr=0.001)\n",
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "# We're in the featureless setting, so each entity is one-hot encoded, hence\n",
    "# the input data is simply the identity matrix of dim N_entities x N_entities\n",
    "I = tensor.sparse_eye(n_entities)\n",
    "\n",
    "#\n",
    "# Training\n",
    "#\n",
    "EPOCHS = 10\n",
    "for i in range(EPOCHS):\n",
    "    print(f\"Step {i+1}/{EPOCHS}\")\n",
    "    optim.zero_grad()\n",
    "    y_pred = rgcn(I)\n",
    "    loss = cross_entropy(y_pred, y_true)\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
