{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Équation générale :\n",
    "$$h_i^{(l+1)} = \\sigma\\left( \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_i^r} \\frac{1}{c_{i,r}} W_r^{(l)}h_j^{(l)} + W_0^{(l)} h_i^{(l)} \\right)$$\n",
    "avec $h_i^{(l)} \\in \\mathbb{R}^{d^{(l)}}$ the hidden state of node $v_i$\n",
    "\n",
    "*basis decomposition*:\n",
    "$$W_r^{(l)} = \\sum_{b=1}^B a_{rb}^{(l)} V_b^{(l)}$$\n",
    "\n",
    "and $V_b^{(l)} \\in \\mathbb{R}^{d^{(l+1)} \\times d^{(l)}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notons $N=|\\mathcal{E}|$, on a :\n",
    "\n",
    "au layer $(l)$, on a une matrice $H^{(l)}$ de dimension $N \\times d^{(l)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = [\n",
    "    (0, 0, 1),\n",
    "    (1, 0, 2),\n",
    "    (2, 0, 0),\n",
    "    (0, 1, 1),\n",
    "    (1, 1, 0),\n",
    "    (0, 2, 2),\n",
    "    (1, 2, 2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.],\n",
      "        [ 1.],\n",
      "        [-1.]])\n",
      "tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [1.5000]])\n",
      "tensor([[1.5000],\n",
      "        [0.0000],\n",
      "        [0.0000]])\n",
      "tensor([[0.0000],\n",
      "        [3.0000],\n",
      "        [0.7500]])\n",
      "tensor([[3.7500],\n",
      "        [0.0000],\n",
      "        [4.5000]])\n",
      "tensor([[4.5000],\n",
      "        [7.5000],\n",
      "        [1.8750]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(self, T, B, dim_in, dim_out, init=\"random\"):\n",
    "        \"\"\"\n",
    "        T: adjacency tensor (Nr * Ne * Ne)\n",
    "        B: number of basis functions\n",
    "        dim_in: dimension of input feature vectors\n",
    "        dim_out: dimension of output feature vectors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        Nr, Ne, _ = T.shape()\n",
    "        self.V = self.init(B, dim_in, dim_out, how=init)\n",
    "        self.A = self.init(Nr, B, how=init)\n",
    "        self.T = T\n",
    "        \n",
    "    def init(self, *size, how=\"random\", fill_value=1.):\n",
    "        if how == \"random\":\n",
    "            return torch.rand(*size)\n",
    "        elif how == \"constant\":\n",
    "            return torch.full(size, fill_value)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported initialization method '{how}'\")\n",
    "            \n",
    "    def forward(self, H):\n",
    "        # Input: N * d_in\n",
    "        W = torch.einsum(\"rb,bio->rio\", self.A, self.V) # -> \"R * d_in * d_out\"\n",
    "        HW = torch.einsum(\"ni,rio->rno\", H, W)\n",
    "        H = torch.einsum(\"rmn,rno->mo\", self.T.to_dense(), HW)\n",
    "        return H\n",
    "    \n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, T, n_classes):\n",
    "        self.conv1 = RGCNLayer(T, 10, 16, 32)\n",
    "        self.conv2 = RGCNLayer(T, 10, 32, n_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "Ne = 3\n",
    "Nr = 3\n",
    "\n",
    "T = get_adjacency(triples, Nr, Ne)\n",
    "\n",
    "B = 1\n",
    "din = 1\n",
    "dout = 1\n",
    "\n",
    "H = torch.Tensor([[0], [1], [-1]])\n",
    "\n",
    "layer = RGCNLayer(T, Nr, B, din, dout, init=\"constant\")\n",
    "\n",
    "print(H)\n",
    "for _ in range(5):\n",
    "    H = layer.forward(H)\n",
    "    print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([[0, 10, 1], [0, -10, -1]])\n",
    "print(A.shape, V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 10.1924,  10.6328,  10.6583,  10.8340],\n",
       "         [ 10.9752,  10.5400,  10.9157,  10.2023]],\n",
       "\n",
       "        [[-10.1924, -10.6328, -10.6583, -10.8340],\n",
       "         [-10.9752, -10.5400, -10.9157, -10.2023]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"rb,bio->rio\", A, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenseur d'adjacence :\n",
    "- dimensions $R \\times N \\times N$\n",
    "- une slice $k$ = la matrice d'adjacence pour la relation $k$. *i.e* $T_{ij}^{(k)} = 1$ ssi $(e_i, r_k, e_j)$ est dans le graphe\n",
    "- pour une slice $(r)$ donnée, la somme sur une ligne doit sommer à 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 1.0000],\n",
       "         [1.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 1.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 1.0000, 0.0000],\n",
       "         [1.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tenseur d'adjacence\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_adjacency(triples, n_rels, n_ents):\n",
    "    d = defaultdict(lambda:defaultdict(set))\n",
    "    for h, r, t in triples:\n",
    "        d[r][t].add(h)\n",
    "    i = []\n",
    "    v = []\n",
    "    for r in d:\n",
    "        for t in d[r]:\n",
    "            n = 1 / len(d[r][t])\n",
    "            for h in d[r][t]:\n",
    "                i.append([r, t, h])\n",
    "                v.append(n)\n",
    "    i = torch.LongTensor(i)\n",
    "    v = torch.FloatTensor(v)\n",
    "    return torch.sparse.FloatTensor(i.t(), v, torch.Size([n_rels, n_ents, n_ents]))\n",
    "\n",
    "# nb d'entités\n",
    "Ne = 3\n",
    "Nr = 3\n",
    "triples = [\n",
    "    (0, 0, 1),\n",
    "    (1, 0, 2),\n",
    "    (2, 0, 0),\n",
    "    (0, 1, 1),\n",
    "    (1, 1, 0),\n",
    "    (0, 2, 2),\n",
    "    (1, 2, 2)\n",
    "]\n",
    "\n",
    "T = get_adjacency(triples, 3, 3)\n",
    "T.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/m/08966',\n",
       "  '/travel/travel_destination/climate./travel/travel_destination_monthly_climate/month',\n",
       "  '/m/05lf_'],\n",
       " ['/m/01hww_',\n",
       "  '/music/performance_role/regular_performances./music/group_membership/group',\n",
       "  '/m/01q99h'],\n",
       " ['/m/09v3jyg',\n",
       "  '/film/film/release_date_s./film/film_regional_release_date/film_release_region',\n",
       "  '/m/0f8l9c'],\n",
       " ['/m/02jx1', '/location/location/contains', '/m/013t85'],\n",
       " ['/m/02jx1', '/location/location/contains', '/m/0m0bj'],\n",
       " ['/m/02bfmn', '/film/actor/film./film/performance/film', '/m/04ghz4m'],\n",
       " ['/m/05zrvfd',\n",
       "  '/award/award_category/nominees./award/award_nomination/nominated_for',\n",
       "  '/m/04y9mm8'],\n",
       " ['/m/060bp',\n",
       "  '/government/government_office_category/officeholders./government/government_position_held/jurisdiction_of_office',\n",
       "  '/m/04j53'],\n",
       " ['/m/07l450', '/film/film/genre', '/m/082gq'],\n",
       " ['/m/07h1h5',\n",
       "  '/sports/pro_athlete/teams./sports/sports_team_roster/team',\n",
       "  '/m/029q3k']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dirname = r\"C:\\Users\\felix\\Downloads\\FB15K-237.2\\Release\"\n",
    "def readf(fname):\n",
    "    with open(os.path.join(dirname, fname), \"r\") as f:\n",
    "        triples = [line.split() for line in f]\n",
    "    return triples\n",
    "\n",
    "triples = readf(\"test.txt\")\n",
    "triples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "[[0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "[[0 1 1]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, diags\n",
    "\n",
    "graph = [triple.split() for triple in \"\"\"\\\n",
    "a r1 b\n",
    "b r1 c\n",
    "c r1 a\n",
    "a r2 b\n",
    "b r2 a\n",
    "a r3 c\n",
    "b r3 c\n",
    "c r4 a\n",
    "c r4 b\n",
    "c r4 c\"\"\".split(\"\\n\")]\n",
    "\n",
    "def adj(triples):\n",
    "    hs, rs, ts = zip(*triples)\n",
    "    entities = {e: i for i, e in enumerate(set(hs) | set(ts))}\n",
    "    relations = {r: i for i, r in enumerate(set(rs))}\n",
    "    nr, ne = len(relations), len(entities)\n",
    "    sorted_triples = defaultdict(list)\n",
    "    for h, r, t in triples:\n",
    "        sorted_triples[relations[r]].append((entities[t], entities[h]))\n",
    "    A = []\n",
    "    for r, coords in sorted_triples.items():\n",
    "        row_inds, col_inds = zip(*coords)\n",
    "        data = [1] * len(coords)\n",
    "        A.append(csr_matrix((data, (row_inds, col_inds)), shape=(ne, ne)))\n",
    "    return A\n",
    "\n",
    "A = adj(graph)\n",
    "for a in A:\n",
    "    print(a.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0. , 1. , 0. , 0. , 0. , 0. , 0. , 0.5, 0.5, 1. , 0. , 0. ],\n",
       "        [0. , 0. , 1. , 0. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 1. , 0. , 0. ]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "def normalize(a):\n",
    "    d = np.array(a.sum(1)).squeeze()\n",
    "    d = np.divide(1, d, where=d!=0)\n",
    "    d = diags(d, format=\"csr\")\n",
    "    return d * a\n",
    "\n",
    "Ah = [normalize(a) for a in A]\n",
    "\n",
    "sparse.hstack(Ah).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = np.array(A[2].sum(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr = len(A)\n",
    "Ne = A[0].shape[0]\n",
    "di = 8\n",
    "do = 16\n",
    "H = torch.rand(Ne, di)\n",
    "W = torch.rand(Nr, di, do)\n",
    "\n",
    "HxW = torch.matmul(H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(Nr, Ne, Ne)\n",
    "\n",
    "AxHxW = torch.matmul(A, HxW)\n",
    "print(AxHxW.shape)\n",
    "\n",
    "AxHxW.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adj(triples):\n",
    "    hs, rs, ts = zip(*triples)\n",
    "    entities = {e: i for i, e in enumerate(set(hs) | set(ts))}\n",
    "    relations = {r: i for i, r in enumerate(set(rs))}\n",
    "    nr, ne = len(relations), len(entities)\n",
    "    i = torch.LongTensor([\n",
    "        [relations[r] for r in rs],\n",
    "        [entities[t] for t in ts],\n",
    "        [entities[h] for h in hs]\n",
    "    ])\n",
    "    c = torch.ones(len(triples))\n",
    "    A = torch.sparse.FloatTensor(i, c, torch.Size([nr, ne, ne]))\n",
    "    return A\n",
    "\n",
    "def adj(triples):\n",
    "    hs, rs, ts = zip(*triples)\n",
    "    entities = {e: i for i, e in enumerate(set(hs) | set(ts))}\n",
    "    relations = {r: i for i, r in enumerate(set(rs))}\n",
    "    nr, ne = len(relations), len(entities)\n",
    "    sorted_triples = defaultdict(list)\n",
    "    for h, r, t in triples:\n",
    "        sorted_triples[relations[r]].append((entities[t], entities[h]))\n",
    "    A = []\n",
    "    for r, coords in sorted_triples.items():\n",
    "        row_inds, col_inds = zip(*coords)\n",
    "        data = [1] * len(coords)\n",
    "        A.append(\n",
    "            torch.sparse.FloatTensor(\n",
    "                torch.LongTensor([row_inds, col_inds]),\n",
    "                torch.ones(len(coords)),\n",
    "                torch.Size([ne, ne])\n",
    "            )\n",
    "        )\n",
    "    return A\n",
    "\n",
    "# shape: Nr * Ne * d_out\n",
    "HxW = torch.matmul(H, W)\n",
    "# shape: Nr * Ne * Ne\n",
    "A = adj(graph)\n",
    "\n",
    "# expected shape: Nr * Ne * d_out\n",
    "AxHxW = torch.stack([torch.sparse.mm(a, hw) for a, hw in zip(A, HxW)])\n",
    "print(AxHxW.shape)\n",
    "\n",
    "AxHxW.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-a993f145803a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mAxHxW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "AxHxW.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
